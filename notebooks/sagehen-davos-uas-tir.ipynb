{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook for UAS TIR imagery analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from scipy import stats\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "#### Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linfit_conf(x,y,alpha,sided=2):\n",
    "    '''compute confidence intervals for a linear regression \n",
    "    line given an alpha and one or two sided test'''\n",
    "    # Modified code from: https://tomholderness.wordpress.com/2013/01/10/confidence_intervals/\n",
    "    # linfit.py - example of confidence limit calculation for linear regression fitting.\n",
    "    \n",
    "    # References:\n",
    "    # - Statistics in Geography by David Ebdon (ISBN: 978-0631136880)\n",
    "    # - Reliability Engineering Resource Website:\n",
    "    # - http://www.weibull.com/DOEWeb/confidence_intervals_in_simple_linear_regression.htm\n",
    "    # - University of Glascow, Department of Statistics:\n",
    "    # - http://www.stats.gla.ac.uk/steps/glossary/confidence_intervals.html#conflim\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # fit a line to the data using a least squares 1st order polynomial fit\n",
    "    z = np.polyfit(x,y,1)\n",
    "    p = np.poly1d(z)\n",
    "    fit = p(x)\n",
    "\n",
    "    # get the coordinates for the fit line\n",
    "    c_y = [np.min(fit),np.max(fit)]\n",
    "    c_x = [np.min(x),np.max(x)]\n",
    " \n",
    "    # predict y values of origional data using the fit\n",
    "    p_y = z[0] * x + z[1]\n",
    "\n",
    "    # calculate the y-error (residuals)\n",
    "    y_err = y - p_y\n",
    " \n",
    "    # create series of new test x-values to predict for\n",
    "    p_x = np.linspace(np.min(x),np.max(x),30)\n",
    " \n",
    "    # now calculate confidence intervals for new test x-series\n",
    "    mean_x = np.mean(x)                     # mean of x\n",
    "    n = len(x)                              # number of samples in origional fit\n",
    "    t = stats.t.ppf(1-(alpha/sided), n-1)   # find t value (1 or 2 sided)\n",
    "    s_err = np.sum(np.power(y_err,2))       # sum of the squares of the residuals\n",
    " \n",
    "    confs = t * np.sqrt((s_err/(n-2))*(1.0/n + (np.power((p_x-mean_x),2)/\n",
    "                ((np.sum(np.power(x,2)))-n*(np.power(mean_x,2))))))\n",
    " \n",
    "    # now predict y based on test x-values\n",
    "    p_y = z[0] * p_x + z[1]\n",
    " \n",
    "    # get lower and upper confidence limits based on predicted y and confidence intervals\n",
    "    lower = p_y - abs(confs)\n",
    "    upper = p_y + abs(confs)\n",
    " \n",
    "    return [p_x, lower, upper]\n",
    "\n",
    "def quick_lin(x,y,color='r',alpha=0.01,print_output=True):\n",
    "    '''computes and plots a simple linear regression line\n",
    "    and tests for statistical significance, using SciPy'''\n",
    "    \n",
    "    ## Pearson's r (parametric correlation test):\n",
    "    pearsons_r = stats.pearsonr(x, y)\n",
    "    \n",
    "    ## Kendall's tau (non-parametric correlation test):\n",
    "    kendalls_tau = stats.kendalltau(x, y)\n",
    "    \n",
    "    # Linear Regression (parametric test for slope = 0):  \n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html\n",
    "    # performs a Wald Test against null hypothesis that slope is zero -- https://en.wikipedia.org/wiki/Wald_test\n",
    "    # See also: https://github.com/scipy/scipy/issues/7074\n",
    "    slope, intercept, r, p, SE = stats.linregress(x,y)\n",
    "    r2 = r**2\n",
    "    \n",
    "    # null hypothesis: slope (dy/dx) is zero, that is, y does not change as a function of x\n",
    "    slope0 = 0; \n",
    "    # alternative hypothesis: slope is non-zero (either positive or negative, two-tailed test)\n",
    "    # Student's t-test (parametric test): https://en.wikipedia.org/wiki/Student%27s_t-test\n",
    "    t_value = ((slope - (slope0))/SE) # SE = standard error\n",
    "    n = len(x)\n",
    "    # P-value from Student's t-test\n",
    "    p_value = stats.t.sf(np.abs(t_value), n-1)*2\n",
    "    # Reject the null hypothesis if p_value <= alpha\n",
    "      \n",
    "    # confidence intervals\n",
    "    alpha = 0.05\n",
    "    [p_x, lower, upper] = linfit_conf(x,y,alpha)\n",
    "        \n",
    "    # Plot the linear fit\n",
    "    x_lin = np.linspace(min(x),max(x),n)\n",
    "    plt.plot(x_lin, slope*x_lin + intercept, ':', c=color, label='Linear Regression')\n",
    "    plt.fill_between(p_x,lower,upper,color=color,alpha=0.1,label='95% Confidence')\n",
    "    #plt.legend()\n",
    "    \n",
    "    # calculate the residuals\n",
    "    residuals = y - (slope*x_lin + intercept) # residuals = measured - model\n",
    "    \n",
    "    # print output if flagged True\n",
    "    if print_output == True:\n",
    "        print(\"Linear regression:\\n\\tslope=%s\\n\\tintercept=%s\\n\\tr=%s\\n\\tr2=%s\\n\\tp-value=%s\\n\\tSE=%s\" \\\n",
    "          % (str(slope), str(intercept), str(r), str(r2), str(p), str(SE)))\n",
    "        print(\"Student's t-test: \\n\\tp-value=%s\" % str(p_value))\n",
    "        print(\"Pearson's r:\\n\\tcorr_coeff=%s\\n\\tp-value=%s\" \\\n",
    "          % (str(pearsons_r[0]), str(pearsons_r[1])))\n",
    "        print(\"Kendalls tau:\\n\\t%s\" % str(kendalls_tau))\n",
    "    \n",
    "    return slope, r, p, SE, t_value, p_value, residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_px(image):\n",
    "    '''counts the number of pixels in an image (or the number of elements in any n-D array)'''\n",
    "    n_pixels = 1\n",
    "    for dim in np.shape(image): \n",
    "        n_pixels *= dim\n",
    "    return n_pixels\n",
    "\n",
    "def rmse(value, predictions):\n",
    "    '''root mean square error'''\n",
    "    error = predictions - value\n",
    "    SE = error**2\n",
    "    MSE = np.mean(SE)\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    mean_bias = np.mean(error) # mean of (predictions - value)\n",
    "    return RMSE, mean_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_histogram(image, n_bins=None, low_threshold=None, high_threshold=None):\n",
    "    '''creates a single image's histogram  '''\n",
    "    # reshape the image pixel array\n",
    "    image = image.reshape(-1);\n",
    "    # remove NaN values\n",
    "    image = image[~np.isnan(image)];\n",
    "    # apply low_threshold:\n",
    "    if low_threshold is not None:\n",
    "        image = image[image>low_threshold]\n",
    "    if high_threshold is not None:\n",
    "        image = image[image<high_threshold]\n",
    "    # create the histogram:\n",
    "    if n_bins is None:\n",
    "        n_bins = int(np.round(max(image) - min(image),0)) # bin width should be set to accuracy of the camera? +/- 1 C\n",
    "    count, value = np.histogram(image, bins=n_bins); \n",
    "    # take the bin edges (value) and change them to bin centers\n",
    "    # so that number of \"count\" = number of \"value\"\n",
    "    new_value = [(i+value[idx+1])/2 for idx, i in enumerate(value) if idx < len(value)-1]\n",
    "    value = new_value\n",
    "    count_stddev = np.std(count)\n",
    "    return [value, count]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radiometric_correction(data, image, Tss):\n",
    "    '''Radiometric correction using a known snow surface temperature (Tss)\n",
    "       Corrects some original array (data) using a histogram based on a \n",
    "       subset (image) and known, uniform, snow surface temperature (Tss)'''\n",
    "    # if data == image then the radiometric correction is performed based on the entire data array\n",
    "    image = image[~np.isnan(image)]; # this should take care of the flattening of the array itself\n",
    "    # find peaks (modes) in the histogram:\n",
    "    value, count = create_histogram(image,int(np.sqrt(len(image)))) #255\n",
    "    count_stddev = np.std(count)\n",
    "    # identify which one is the the \"snow peak\" -- assumed to be the coldest peak in the image's histogram\n",
    "    modes = []\n",
    "    counts = []\n",
    "    if sagehen == True:\n",
    "        for i in range(0,len(count)-1):\n",
    "            if count[i]-np.mean(count) > count_stddev:\n",
    "                # if the difference between the peak the local mean is > 1 sigma, then we'll call this a peak\n",
    "                if i > 0:\n",
    "                    if count[i-1] < count[i] and count[i+1] < count[i]:\n",
    "                        # if the count at this index is a local maximum based on its two neighbors\n",
    "                        #print('peak at i=',i)\n",
    "                        modes.append(value[i])\n",
    "                        counts.append(count[i])\n",
    "                else:\n",
    "                    if count[i+1] < count[i]:\n",
    "                        # if the count at this index (0th index) is a local maximum based on its one neighbor\n",
    "                        #print('peak at i=',i)\n",
    "                        modes.append(value[i])\n",
    "                        counts.append(count[i])\n",
    "    if sagehen == False:\n",
    "        for i in range(1,len(count)-1):\n",
    "            if count[i-1] < count[i] and count[i+1] < count[i]:\n",
    "                # if the count at this index is a local maximum based on its two neighbors\n",
    "                #if count[i]-np.mean(count) > count_stddev: # and if the difference between the peak the local mean is > 1 sigma, then we'll call this a peak (used for Sagehen)\n",
    "                if value[i] < 0: # and if the peak appears colder than 0 C (using this for Davos right now)    \n",
    "                    #print('peak at i=',i)\n",
    "                    modes.append(value[i])\n",
    "                    counts.append(count[i])\n",
    "\n",
    "    #print('snow peak:', modes[0]) # show us what the coldest peak in the histogram is\n",
    "    \n",
    "    # set the radiometric (\"histogram\") offset to this minimum peak,\n",
    "    # so that the peak will be equal to own known value of Tss\n",
    "    #print(modes)\n",
    "    hist_offset = modes[0] - Tss\n",
    "\n",
    "    # subtract the radiometric offset value from the entire image data\n",
    "    data_corrected = data - hist_offset; \n",
    "    \n",
    "    return data_corrected, hist_offset, modes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSummaryStats(image):\n",
    "    '''compute some stats for a given image'''\n",
    "    i_mean = np.mean(image)\n",
    "    i_median = np.median(image)\n",
    "    i_std = np.std(image)\n",
    "    i_max = np.max(image)\n",
    "    i_min = np.min(image)\n",
    "    i_range = i_max - i_min\n",
    "    \n",
    "    return [i_mean, i_median, i_std, i_max, i_min, i_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upscaling_routines(original_image,px_size,Tss,target_res,make_plots=False,save_plots=False,plot_index=0):\n",
    "    ### Perform the radiometric correction on the original image:\n",
    "    image = original_image.reshape(-1);\n",
    "    ### Perform the radiometric correction of \"data[:,:,img]\"\" based on the histogram of \"image\", and \"Tss\"\n",
    "    image_corrected = radiometric_correction(original_image,image,Tss)\n",
    "    \n",
    "    ### Upscale the image_corrected\n",
    "    # Gaussian Pyramids with OpenCV:\n",
    "    steps = int(np.round(np.log(target_res/px_size)/(np.log(2)))) # how many steps to take to get (close) to target resolution\n",
    "    g_new_res = px_size * 2**steps # what the actual new resolution will be\n",
    "    lower_res = image_corrected # start with the original image, each iteration makes it lower resolution\n",
    "    for i in range(0,steps):\n",
    "        lower_res = cv2.pyrDown(lower_res) # iterate using pyrDown the number of steps we need\n",
    "    gaussian_upscaled = lower_res \n",
    "    \n",
    "    \n",
    "    ### Compare some stats from the original image to the upscaled image(s)\n",
    "    # Original image:\n",
    "    [o_mean, o_median, o_std, o_max, o_min] = getSummaryStats(image_corrected)\n",
    "    o_text = \"Original Image:\\n$\\overline{T}$: %.2f \\n$\\widetilde{T}$: %.2f \\n$\\sigma_{T}$: %.2f \\n$T_{MAX}$: %.2f \\n$T_{MIN}$: %.2f\" % (o_mean, o_median, o_std, o_max, o_min)\n",
    "    # Gaussian upscaled image:\n",
    "    [g_mean, g_median, g_std, g_max, g_min] = getSummaryStats(gaussian_upscaled)\n",
    "    g_text = \"Gaussian Upscaled:\\n$\\overline{T}$: %.2f \\n$\\widetilde{T}$: %.2f \\n$\\sigma_{T}$: %.2f \\n$T_{MAX}$: %.2f \\n$T_{MIN}$: %.2f\" % (g_mean, g_median, g_std, g_max, g_min)\n",
    "    # Simple mean aggregated image:\n",
    "    [m_mean, m_median, m_std, m_max, m_min] = getSummaryStats(mean_agg)\n",
    "    m_text = \"Simple Mean:\\n$\\overline{T}$: %.2f \\n$\\widetilde{T}$: %.2f \\n$\\sigma_{T}$: %.2f \\n$T_{MAX}$: %.2f \\n$T_{MIN}$: %.2f\" % (m_mean, m_median, m_std, m_max, m_min)\n",
    "    # Spatial average aggregated image:\n",
    "    [s_mean, s_median, s_std, s_max, s_min] = getSummaryStats(spatial_average)\n",
    "    s_text = \"Spatial Mean:\\n$\\overline{T}$: %.2f \\n$\\widetilde{T}$: %.2f \\n$\\sigma_{T}$: %.2f \\n$T_{MAX}$: %.2f \\n$T_{MIN}$: %.2f\" % (s_mean, s_median, s_std, s_max, s_min)\n",
    "    \n",
    "    ### Compare some stats from the upscaled image(s) to aircraft imagery\n",
    "    \n",
    "    ### Plots\n",
    "    if make_plots == True:\n",
    "        ### Plot images\n",
    "        plt.figure(figsize=(20,5))\n",
    "        \n",
    "        v_min=0\n",
    "        v_max=25\n",
    "        \n",
    "        plt.subplot(141)\n",
    "        plt.imshow(image_corrected,vmin=v_min,vmax=v_max,cmap='magma')\n",
    "        plt.title(\"original image: \" + str(np.round(px_size,2)))\n",
    "        \n",
    "        plt.subplot(142)\n",
    "        plt.imshow(gaussian_upscaled,vmin=v_min,vmax=v_max,cmap='magma')\n",
    "        plt.title(\"pyrDown upscaled image: \" + str(np.round(g_new_res,2)))\n",
    "        \n",
    "        plt.subplot(143)\n",
    "        plt.imshow(mean_agg,vmin=v_min,vmax=v_max,cmap='magma')\n",
    "        plt.title(\"mean aggregated image: \" + str(np.round(m_new_res,2)))\n",
    "        \n",
    "        plt.subplot(144)\n",
    "        plt.imshow(spatial_average,vmin=v_min,vmax=v_max,cmap='magma')\n",
    "        plt.title(\"spatial average image: \" + str(np.round(m_new_res,2)))\n",
    "        \n",
    "\n",
    "        if save_plots == True:\n",
    "            plt.savefig(r'plots\\upscaling\\\\images_' + str(plot_index) + '.png')\n",
    "        \n",
    "        ### T vs T plots\n",
    "        plt.figure(figsize=(20,5))\n",
    "               \n",
    "        plt.subplot(141)\n",
    "        compareResDeltaHist(image_corrected,image_corrected,lims=(-5,35))\n",
    "        plt.title(\"original image: \" + str(np.round(px_size,2)))\n",
    "        \n",
    "        plt.subplot(142)\n",
    "        compareResDeltaHist(image_corrected,gaussian_upscaled,lims=(-5,35))\n",
    "        plt.title(\"pyrDown upscaled image: \" + str(np.round(g_new_res,2)))\n",
    "        \n",
    "        plt.subplot(143)\n",
    "        compareResDeltaHist(image_corrected,mean_agg,lims=(-5,40))\n",
    "        plt.title(\"mean aggregated image: \" + str(np.round(m_new_res,2)))\n",
    "        \n",
    "        plt.subplot(144)\n",
    "        compareResDeltaHist(image_corrected,spatial_average,lims=(-5,35))\n",
    "        plt.title(\"spatial average image: \" + str(np.round(m_new_res,2)))\n",
    "        \n",
    "        if save_plots == True:\n",
    "            plt.savefig(r'plots\\upscaling\\\\OriginalTvsDeltaT_hist_' + str(plot_index) + '.png')\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### Histogram plots\n",
    "        plt.figure(figsize=(20,5))\n",
    "        original_hist = create_histogram(image_corrected)\n",
    "        plt.plot(original_hist[0],original_hist[1]/np.sum(original_hist[1]),'--k',label='original')\n",
    "        gaussian_hist = create_histogram(gaussian_upscaled)\n",
    "        plt.plot(gaussian_hist[0],gaussian_hist[1]/np.sum(gaussian_hist[1]),':k',label='gaussian pyramid')\n",
    "        mean_agg_hist = create_histogram(mean_agg)\n",
    "        plt.plot(mean_agg_hist[0],mean_agg_hist[1]/np.sum(mean_agg_hist[1]),':r',label='simple average')\n",
    "        spatial_avg_hist = create_histogram(spatial_average)\n",
    "        plt.plot(spatial_avg_hist[0],spatial_avg_hist[1]/np.sum(spatial_avg_hist[1]),':b',label='spatial average')\n",
    "        \n",
    "        plt.text(-5,0.25,o_text)\n",
    "        plt.text(5,0.25,g_text)\n",
    "        plt.text(15,0.25,m_text)\n",
    "        plt.text(25,0.25,s_text)\n",
    "        \n",
    "        plt.ylim((0,0.4))\n",
    "        plt.xlim((-10,40))\n",
    "        \n",
    "        plt.legend()\n",
    "        if save_plots == True:\n",
    "            plt.savefig(r'plots\\upscaling\\\\histogram_' + str(plot_index) + '.png')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three different plot types for comparing an image that's been scaled to a different resolution with its original\n",
    "\n",
    "def compareResScatter(original_img,scaled_img,lims=(-5,35)):\n",
    "    zoom = np.max([original_img.shape[0]/scaled_img.shape[0],original_img.shape[1]/scaled_img.shape[1]])\n",
    "    scaled_original_res = ndimage.zoom(scaled_img, zoom, order=0, mode='constant', cval=0.0, prefilter=True)\n",
    "    scaled_original_res = scaled_original_res[0:original_img.shape[0],0:original_img.shape[1]]\n",
    "    plt.scatter(scaled_original_res,original_img,c='k',s=0.1,alpha=0.1)\n",
    "    # add 1:1 line\n",
    "    plt.plot(np.linspace(lims[0],lims[1],10),np.linspace(lims[0],lims[1],10),':b')\n",
    "    # set axes limits\n",
    "    plt.ylim(lims)\n",
    "    plt.xlim(lims)\n",
    "    plt.ylabel(\"Original T\")\n",
    "    plt.xlabel(\"Scaled T\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def compareResDelta(original_img,scaled_img,lims=(-5,40)):\n",
    "    zoom = np.max([original_img.shape[0]/scaled_img.shape[0],original_img.shape[1]/scaled_img.shape[1]])\n",
    "    scaled_original_res = ndimage.zoom(scaled_img, zoom, order=0, mode='constant', cval=0.0, prefilter=True)\n",
    "    scaled_original_res = scaled_original_res[0:original_img.shape[0],0:original_img.shape[1]]\n",
    "    plt.scatter(original_img,scaled_original_res-original_img,c='k',s=0.1,alpha=0.1)\n",
    "    # add 1:1 line\n",
    "    plt.plot(np.linspace(lims[0],lims[1],10),np.linspace(0,0,10),':b')\n",
    "    # set axes limits\n",
    "    plt.ylim((-30,30))\n",
    "    plt.xlim(lims)\n",
    "    plt.ylabel(\"Scaled T - Original T\")\n",
    "    plt.xlabel(\"Original T\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def compareResDeltaHist(original_img,scaled_img,lims=(-20,20)):\n",
    "    zoom = np.max([original_img.shape[0]/scaled_img.shape[0],original_img.shape[1]/scaled_img.shape[1]])\n",
    "    scaled_original_res = ndimage.zoom(scaled_img, zoom, order=0, mode='constant', cval=0.0, prefilter=True)\n",
    "    scaled_original_res = scaled_original_res[0:original_img.shape[0],0:original_img.shape[1]]\n",
    "    # create and plot histogram\n",
    "    n_bins = 500\n",
    "    count, value = np.histogram(scaled_original_res-original_img, bins=n_bins); \n",
    "    # take the bin edges (value) and change them to bin centers\n",
    "    # so that number of \"count\" = number of \"value\"\n",
    "    new_value = [(i+value[idx+1])/2 for idx, i in enumerate(value) if idx < len(value)-1]\n",
    "    value = new_value\n",
    "    plt.plot(value,count,'-k')\n",
    "    # add 0 line\n",
    "    plt.plot(np.linspace(0,0,10),np.linspace(0,10000,10),':b')\n",
    "    # set axes limits\n",
    "    plt.ylim((0,20000))\n",
    "    plt.xlim((-20,20))\n",
    "    plt.ylabel(\"Pixel Count\")\n",
    "    plt.xlabel(\"Scaled T - Original T\")\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple test for significant change in mean, standard deviation, or range of T:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_for_T_change(px_sizes,means,stddevs,mins,maxs,std_error_mean,std_error,color='k',save_figure=False):\n",
    "    '''Test for significant change in temperature mean, standard deviation, or range of specified class'''\n",
    "    print(\"\\nMean\")\n",
    "    plt.figure(0,figsize=(6,6))\n",
    "    plt.plot(px_sizes,means,'.',c=color, label='$Mean T_{}$',);\n",
    "    plt.errorbar(px_sizes,means, fmt='none', xerr=None, yerr=std_error_mean, ecolor=color, elinewidth=0.5, capsize=None)\n",
    "    result = quick_lin(px_sizes,means,color)\n",
    "    plt.ylabel('$Mean T_{}, C^{O}$');\n",
    "    plt.xlabel('Pixel Size, m')\n",
    "    textstr = '$R^2$=%.2f\\np-value=%e' % (result[1]**2, result[2])\n",
    "    plt.title('$Mean T_{}$, n=%d\\n%s' % (n, textstr));\n",
    "    #props = dict(boxstyle='square', facecolor='lightgrey', alpha=0.5)\n",
    "    #plt.text(1, 0.045, textstr, fontsize=12, \\\n",
    "    #        verticalalignment='top', bbox=props)\n",
    "    mean_residuals = result[6]\n",
    "    ###\n",
    "    plt.xlim((0,0.2))\n",
    "    plt.xticks(np.arange(0, 0.21, 0.05))\n",
    "    plt.ylim((0,16))\n",
    "    ###\n",
    "    if save_figure==True:\n",
    "        plt.savefig(r'plots\\PaperFigures\\meanfig.png',\n",
    "                transparent=True, dpi=300)\n",
    "    ###\n",
    "    print(\"Change in mean (C):\")\n",
    "    print((means[-1]-means[0]))\n",
    "    print(\"Change in  mean (%):\")\n",
    "    print(((means[-1]-means[0])/means[0])*100)\n",
    "\n",
    "\n",
    "    print(\"\\nStandard Deviation\")\n",
    "    plt.figure(1,figsize=(5,5))\n",
    "    plt.plot(px_sizes,stddevs,'.k', label='$\\sigma\\ T_{}$',);\n",
    "    plt.errorbar(px_sizes,stddevs, fmt='none', xerr=None, yerr=std_error, ecolor='k', elinewidth=0.5, capsize=None)\n",
    "    result = quick_lin(px_sizes,stddevs)\n",
    "    plt.ylabel('$T_{} Standard Deviation, C^{O}$');\n",
    "    plt.xlabel('Pixel Size, m')\n",
    "    textstr = '$R^2$=%.2f\\np-value=%e' % (result[1]**2, result[2])\n",
    "    plt.title('Standard Deviation $T_{}$, n=%d\\n%s' % (n, textstr));\n",
    "    #props = dict(boxstyle='square', facecolor='lightgrey', alpha=0.5)\n",
    "    #plt.text(1.5, 0.05, textstr, fontsize=12, \\\n",
    "    #        verticalalignment='top', bbox=props)\n",
    "    std_residuals = result[5]\n",
    "    print(\"Change in stddevs (C):\")\n",
    "    print((stddevs[-1]-stddevs[0]))\n",
    "    print(\"Change in  stddevs (%):\")\n",
    "    print(((stddevs[-1]-stddevs[0])/stddevs[0])*100)\n",
    "\n",
    "    print(\"\\nMin\")\n",
    "    plt.figure(2,figsize=(5,5))\n",
    "    plt.plot(px_sizes,mins,'.k', label='Min $T_{}$',);\n",
    "    plt.errorbar(px_sizes,mins, fmt='none', xerr=None, yerr=std_error, ecolor='k', elinewidth=0.5, capsize=None)\n",
    "    result = quick_lin(px_sizes,mins)\n",
    "    plt.ylabel('$Min T_{}, C^{O}$');\n",
    "    plt.xlabel('Pixel Size, m')\n",
    "    textstr = '$R^2$=%.2f\\np-value=%e' % (result[1]**2, result[2])\n",
    "    plt.title('Min $T_{}$, n=%d\\n%s' % (n, textstr));\n",
    "    #props = dict(boxstyle='square', facecolor='lightgrey', alpha=0.5)\n",
    "    #plt.text(1.5, 0.05, textstr, fontsize=12, \\\n",
    "    #        verticalalignment='top', bbox=props)\n",
    "    min_residuals = result[5]\n",
    "    \n",
    "    print(\"\\nMax\")\n",
    "    plt.figure(3,figsize=(5,5))\n",
    "    plt.plot(px_sizes,maxs,'.k', label='Min $T_{}$',);\n",
    "    plt.errorbar(px_sizes,maxs, fmt='none', xerr=None, yerr=std_error, ecolor='k', elinewidth=0.5, capsize=None)\n",
    "    result = quick_lin(px_sizes,maxs)\n",
    "    plt.ylabel('$Max T_{}, C^{O}$');\n",
    "    plt.xlabel('Pixel Size, m')\n",
    "    textstr = '$R^2$=%.2f\\np-value=%e' % (result[1]**2, result[2])\n",
    "    plt.title('Max $T_{}$, n=%d\\n%s' % (n, textstr));\n",
    "    #props = dict(boxstyle='square', facecolor='lightgrey', alpha=0.5)\n",
    "    #plt.text(1.5, 0.05, textstr, fontsize=12, \\\n",
    "    #        verticalalignment='top', bbox=props)\n",
    "    max_residuals = result[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sensitivity to measurement error\n",
    "def msmtErrorTest(x,y,s,m=100):\n",
    "    # error (s) should be in deg C, standard deviaion of the normally distributed error\n",
    "       \n",
    "    # create data with normally distributed random error around the mean value\n",
    "    y_m = np.ones((m,len(y))) # create array to hold new x values for each of m test runs\n",
    "    slopes = []; r2s = []; ps = []; ps2 = []; # empty array for slope, r, p results from linear regressions\n",
    "    for i in range(0,m):\n",
    "        y_m[i] = np.random.normal(y,s)\n",
    "        plt.plot(x,y_m[i],'sk',alpha=(m*0.1)**-1);\n",
    "        # perform a linear regression on each set to test for significance of slope different than 0\n",
    "        [slope, r, p, SE, t_value, p_value, residuals] = quick_lin(x,y_m[i],'r',0.05,False)\n",
    "        slopes.append(slope);\n",
    "        r2s.append(r**2);\n",
    "        ps.append(p);\n",
    "        ps2.append(p_value);\n",
    "        \n",
    "    # calculate the 95% confidence intervals around each mean value using the test runs above\n",
    "    p95 = np.ones((len(y),2)) # create array to hold the upper and lower 95% confidence intervals for each y value\n",
    "    errorbars = np.ones((len(y),2)) # create array to hold the upper and lower errorbar magnitudes\n",
    "    for i in range (0,len(y)):\n",
    "        conf_upper = np.percentile(y_m[:][i], 97.5)\n",
    "        conf_lower = np.percentile(y_m[:][i], 2.5)\n",
    "        p95[i] = [conf_upper, conf_lower]\n",
    "        errorbars[i] = [conf_upper-y[i], y[i]-conf_lower]\n",
    "    \n",
    "    return y_m, p95, errorbars, slopes, r2s, ps, ps2\n",
    "\n",
    "\n",
    "# Plot the sensitivity to measurement error tests\n",
    "def msmtErrorTest_plots(explanatory_variable,test_variable,s,m=100):\n",
    "\n",
    "    plt.figure(0,figsize=(7,3))\n",
    "    [y_tests, p95, errorbars, slopes, r2s, ps, ps2] = msmtErrorTest(explanatory_variable,test_variable,s,m)\n",
    "    plt.plot(explanatory_variable,test_variable,'ob');\n",
    "    plt.errorbar(explanatory_variable,test_variable, fmt='none', xerr=None, yerr=[errorbars.T[1], errorbars.T[0]], ecolor='b', elinewidth=1, capsize=1)\n",
    "    #plt.plot(px_sizes,p95.T[0],'or')\n",
    "    #plt.plot(px_sizes,p95.T[1],'om')\n",
    "    plt.title('Linear Regression Monte Carlo Tests')\n",
    "    plt.xlabel('explanatory_variable')\n",
    "    plt.ylabel('test_variable')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    bins = int(np.sqrt(m))\n",
    "\n",
    "    plt.figure(1,figsize=(7,7))\n",
    "    plt.subplot(221)\n",
    "    plt.hist(slopes,bins)\n",
    "    plt.plot(np.linspace(0, 0),np.linspace(0,m*.1),':k',label='slope=0')\n",
    "    plt.title('Slope')\n",
    "    plt.xlabel('degrees C / meter (pixel resolution)')\n",
    "    plt.ylabel('number of occurrences')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(222)\n",
    "    plt.hist(r2s,bins)\n",
    "    plt.title('Coefficient of Determination (r2)')\n",
    "    plt.xlabel('coefficient of determination (r2)')\n",
    "    plt.ylabel('number of occurrences')\n",
    "\n",
    "    # If i'm doing a 2-sided test with alpha=5%, then reject the null (no slope) if p <= alpha/2\n",
    "    a_2 = 0.025\n",
    "\n",
    "    plt.subplot(223)\n",
    "    plt.hist(ps,bins)\n",
    "    plt.plot(np.linspace(a_2, a_2),np.linspace(0,m*.25),':k',label='alpha')\n",
    "    plt.legend()\n",
    "    plt.title('Wald Test p-value')\n",
    "    plt.xlabel('p-value')\n",
    "    plt.ylabel('number of occurrences')\n",
    "\n",
    "    plt.subplot(224)\n",
    "    plt.hist(ps2,bins)\n",
    "    plt.plot(np.linspace(a_2, a_2),np.linspace(0,m*.25),':k',label='alpha')\n",
    "    plt.legend()\n",
    "    plt.title('Students t-test p-value')\n",
    "    plt.xlabel('p-value')\n",
    "    plt.ylabel('number of occurrences')\n",
    "    plt.tight_layout()\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhist(x, n_bins=None,lower_limit=-2,upper_limit=25):\n",
    "    if n_bins is None:\n",
    "        n_bins = int(np.round(np.max(x) - np.min(x),0)) # bin width default to +/- 1\n",
    "    count, value = np.histogram(x, bins=n_bins); \n",
    "    # take the bin edges (value) and change them to bin centers\n",
    "    # so that number of \"count\" = number of \"value\"\n",
    "    new_value = [(i+value[idx+1])/2 for idx, i in enumerate(value) if idx < len(value)-1]\n",
    "    value = new_value\n",
    "       \n",
    "        \n",
    "    median_x = np.median(x)\n",
    "    mean_x = np.mean(x)\n",
    "    mode_x = value[count.argmax()]  \n",
    "    \n",
    "    print('Mean:\\t%f\\nMedian:\\t%f\\nMode:\\t%f' % (mean_x, median_x, mode_x))\n",
    "        \n",
    "    \n",
    "    fig = plt.figure(figsize=(5,5),frameon=False)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    #plt.hist(x,bins=n_bins,label='Temperature Histogram',color='lightgray');\n",
    "    ax.fill_between(value,count,0,color='0.5',alpha=0.1)\n",
    "    ax.plot(np.linspace(0,0,10),np.linspace(0,np.max(count)+1,10),'-',c='lightgray',label='0 C', linewidth=1)\n",
    "    ax.plot(value,count,'-k', linewidth=1)\n",
    "    #ax.plot(np.linspace(median_x,median_x,10),np.linspace(0,15,10),'--k',label='Median Temp.')\n",
    "    #ax.plot(np.linspace(mean_x,mean_x,10),np.linspace(0,15,10),'-k',label='Mean Temp.')\n",
    "    #ax.plot(np.linspace(np.min(value),np.max(value),10),np.linspace(0,0,10),':k',label='0 C')\n",
    "    #ax.text(mean_x,14,str(np.round(mean_x,2)))\n",
    "    #ax.text(median_x,12,str(np.round(median_x,2)))\n",
    "    plt.xlabel('Temperature (C)')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    ax.xaxis.grid(which='major', color='lightgray', linestyle=':', linewidth=0.7)\n",
    "    \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    #ax.spines['bottom'].set_visible(False)\n",
    "    #ax.spines['left'].set_visible(False)\n",
    "    plt.ylim((0,np.max(count)+1))\n",
    "    plt.xlim((lower_limit,upper_limit))\n",
    "    plt.tight_layout()\n",
    "    #plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load IR images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to directory for the vertical flight TIR image csv files\n",
    "\n",
    "# Sagehen:\n",
    "sagehen = True;\n",
    "path = r'data\\ici_csvs\\vertical_day1'\n",
    "\n",
    "\n",
    "# Davos:\n",
    "#sagehen = False;\n",
    "#path = r'DFlights_20170327\\Flight_1\\Thermal with Heights'\n",
    "#path = r'Flights_20170327\\Flight_2\\Thermal with Heights'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in the UAS TIR images\n",
    "\n",
    "# file extension to detect in this directory\n",
    "extension = '.csv'\n",
    "\n",
    "# load all the TIR image files into an array (called \"data\")\n",
    "print(\"Loading files...\\n\")\n",
    "file = 1;\n",
    "for root, dirs_list, files_list in os.walk(path):\n",
    "    for file_name in files_list:\n",
    "        if os.path.splitext(file_name)[-1] == extension:\n",
    "            file_name_path = os.path.join(root, file_name)\n",
    "            #print(file_name)\n",
    "            print(file_name_path)   # This is the full path of the file\n",
    "            if sagehen == True: d = ','\n",
    "            if sagehen == False: d = ';'\n",
    "            current_image = genfromtxt(file_name_path, delimiter=d)\n",
    "            if sagehen == True: #only do this for sagehen images\n",
    "                # remove the last column of NaN values that genfromtxt is pulling in from some of the csv files\n",
    "                original_shape = (512,640)\n",
    "                #current_image = current_image[:,0:-1]\n",
    "                current_image = current_image[~np.isnan(current_image)]\n",
    "                current_image = np.reshape(current_image,original_shape)\n",
    "            if sagehen == False: # check to see if I also need to remove nans from the Davos imagery\n",
    "                original_shape= (288, 382)\n",
    "                current_image = current_image[~np.isnan(current_image)]\n",
    "                current_image = np.reshape(current_image,original_shape)\n",
    "            if file == 1:\n",
    "                # if this is the first image, then create the data array with the first image\n",
    "                data = current_image\n",
    "            else:\n",
    "                # if this is not the first image, add this new image to the array containing the previous ones\n",
    "                data = np.dstack((data,current_image))\n",
    "            file = file + 1; # increment the file counter\n",
    "\n",
    "nfiles = file\n",
    "print(nfiles)\n",
    "            \n",
    "original_data = np.copy(data); # keep a copy of the unaltered data for later comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load corresponding flight log data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sagehen == True:\n",
    "    # Sagehen:\n",
    "    # Load data from flight log file: verticalCamTriggerInfo.csv\n",
    "    flight_log_file = r'data\\auxilary\\verticalCamTriggerInfo.csv'\n",
    "    # lambda function for reading datestamps\n",
    "    str2date = lambda x: dt.strptime(x.decode(\"utf-8\"), '%Y-%m-%d %H:%M:%S.SSS')\n",
    "    # read the file\n",
    "    flight_log = genfromtxt(flight_log_file, delimiter=',', skip_header=0, names=True, converters = {'datetime': str2date})\n",
    "    \n",
    "if sagehen == False:\n",
    "    # Davos:\n",
    "    # Load data from flight altitude log file:\n",
    "    flight_log_file = r'Flights_20170327\\Flight_1\\Thermal with Heights\\Flights_20170327_Flight1_Heights_70images.txt'\n",
    "    #flight_log_file = r'Flights_20170327\\Flight_2\\Thermal with Heights\\Flights_20170327_Flight2_Heights_70images.txt'\n",
    "    # read the file\n",
    "    flight_log = genfromtxt(flight_log_file, dtype=[('Filename', '<S30'), ('Height_above_reference_m', 'i8')], delimiter='\\t', skip_header=0, names=True)\n",
    "\n",
    "print(\"Loaded:\")\n",
    "print(flight_log_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve flight altitudes and set camera geometry variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get flight altitude for each image (in different columns for the two data sets, specified with \"c\")\n",
    "if sagehen == True: c = 9;\n",
    "if sagehen == False: c = 1;\n",
    "img_heights = [];\n",
    "for i in range(0,data.shape[2]):\n",
    "    img_heights.append(flight_log[i][c]);\n",
    "\n",
    "\n",
    "# camera FOV (degrees) and image dimensions (pixels):\n",
    "if sagehen == True:\n",
    "    fov_x = 50\n",
    "    fov_y = 37.5\n",
    "    pix_x = 640\n",
    "    pix_y = 512\n",
    "if sagehen == False:\n",
    "    fov_x = 38\n",
    "    fov_y = 29\n",
    "    pix_x = 382\n",
    "    pix_y = 288"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define area of interest (AOI) bounding boxes around target tree in each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate temperature distributions across selected areas of interest (AOI)\n",
    "# Each AOI defined as a rectangle in each image: [top left pixel X, top teft pixel Y, width, height]\n",
    "# Images corresponding with the AOIs are specified by image indices in \"images\"\n",
    "\n",
    "# Sagehen:\n",
    "sagehen_images = list(range(5,46,1))\n",
    "sagehen_aoi = [\n",
    "    [33,270,180,180], [20,270,180,180], [80,260,140,140], [70,270,140,140], [60,275,137,137], [133,260,107,107], \n",
    "    [133,250,107,107], [138,250,107,107], [160,188,95,95], [175,180,97,97], [180,177,97,97], [190,168,87,87], \n",
    "    [190,158,85,85], [195,154,86,86], [180,160,82,82], [220,205,82,82], [160,112,78,78], [510,140,86,86], \n",
    "    [507,150,87,87], [499,159,86,86], [483,155,85,85], [480,140,82,82], [479,160,74,74], [488,187,76,76], \n",
    "    [502,180,75,75], [486,135,79,79], [481,149,79,79], [458,155,69,69], [470,160,68,68], [469,140,68,68], \n",
    "    [477,162,68,68], [458,163,67,67], [464,163,60,60], [468,166,60,60], [453,172,60,60], [465,170,60,60],\n",
    "    [455,165,57,57], [448,165,55,55], [452,171,55,55], [442,172,55,55], [444,165,55,55]\n",
    "]\n",
    "\n",
    "# AOI for flight 1:\n",
    "davos_images_1 = list(range(0,nfiles-1))\n",
    "davos_aoi_1 = [\n",
    "    [75,0,307,287], [75,20,290,290], [75,30,270,270], [75,30,270,270], [75,30,270,270], [75,45,265,265], [78,40,250,250], \n",
    "    [78,45,240,240], [78,45,235,235], [80,45,230,230], [80,45,225,225], [80,45,225,225], [88,45,210,210], \n",
    "    [90,50,200,200], [100,60,195,195], [100,60,190,190], [100,60,185,185], [105,60,180,180], [110,60,180,180], \n",
    "    [105,60,175,175], [105,60,175,175], [105,75,175,175], [100,75,175,175], [100,75,175,175], [105,75,170,170], \n",
    "    [110,75,160,160], [110,75,150,150], [115,75,145,145], [115,75,140,140], [115,80,140,140], [125,75,135,135], \n",
    "    [125,75,135,135], [125,75,135,135], [125,75,135,135], [125,80,135,135], [125,75,130,130], [125,75,130,130], \n",
    "    [125,80,130,130], [125,80,130,130], [130,85,125,125], [130,85,120,120], [135,85,115,115], [130,85,115,115], \n",
    "    [135,85,110,110], [135,80,110,110], [130,85,110,110], [130,85,110,110], [130,85,110,110], [135,85,110,110], \n",
    "    [140,90,105,105], [140,85,105,105], [140,85,105,105], [140,90,105,105], [145,90,100,100], [145,90,100,100], \n",
    "    [145,90,100,100], [145,90,95,95], [150,95,90,90], [150,95,90,90], [150,100,90,90], [150,100,90,90], \n",
    "    [150,100,90,90], [150,100,85,85], [150,100,85,85], [150,100,85,85], [155,100,85,85], [155,100,80,80], \n",
    "    [155,100,80,80], [160,100,80,80], [160,100,80,80]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# AOI for flight 2:\n",
    "davos_images_2 = list(range(0,nfiles-1))\n",
    "davos_aoi_2 = [\n",
    "    [175,60,50,80], [175,60,50,80], [175,60,50,80], [175,60,50,80], [175,60,50,80], [175,60,50,80], [175,55,50,80], \n",
    "    [175,55,50,85], [175,55,50,85], [175,60,50,85], [175,60,50,85], [175,60,50,85], [175,60,50,85], [175,60,50,85], \n",
    "    [175,60,50,85], [175,60,50,85], [175,60,50,85], [170,60,55,90], [170,60,55,90], [170,60,55,90], [170,60,55,90], \n",
    "    [170,60,55,95], [165,60,55,95], [165,60,60,95], [165,60,60,95], [165,60,65,95], [170,60,60,95], [170,55,65,100], \n",
    "    [170,55,65,100], [165,55,70,100], [165,55,65,105], [165,55,70,105], [160,50,70,110], [160,60,70,115], \n",
    "    [165,50,70,120], [165,50,70,120], [165,55,70,125], [165,55,75,125], [165,55,75,125], [165,55,80,125], \n",
    "    [165,50,80,130], [165,50,80,130], [165,50,80,130], [165,50,80,140], [160,45,90,145], [155,45,90,150], \n",
    "    [155,45,95,150], [160,40,90,155], [160,30,95,160], [160,30,105,165], [160,30,105,170], [160,30,105,175], \n",
    "    [160,30,105,180], [155,30,105,180], [150,30,105,180], [150,30,110,180], [150,25,120,190], [145,25,120,200], \n",
    "    [145,25,120,200], [145,15,125,210], [145,10,130,215], [140,5,135,220], [140,5,140,245], [140,5,145,245], \n",
    "    [140,0,150,260], [135,0,160,265], [125,0,165,300], [125,0,175,300], [125,0,175,300], [120,0,188,300]\n",
    "]\n",
    "\n",
    "# choose one of the above three sets of pixel coordinates and image lists:\n",
    "aoi = sagehen_aoi;\n",
    "images = sagehen_images;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up options for main analysis routines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figures = False\n",
    "save_figures = False\n",
    "\n",
    "\n",
    "# Apply radiometric correction?\n",
    "# If we want to introduct a lag to our data correction, make sure to disable rad_correction\n",
    "rad_correction = True\n",
    "rad_correction_aoi = True # if we want to only look at the area of interest defined by the bounding boxes\n",
    "\n",
    "# make empty array to hold the corrected data values\n",
    "data_corrected = np.empty(data.shape);\n",
    "hist_offset = np.empty(len(images));\n",
    "snowpeak = np.empty(len(images));\n",
    "\n",
    "# make empty arrays to hold the AOI mean, median, std values, altitude and px sizes\n",
    "img_means_aoi = [];\n",
    "img_medians_aoi = [];\n",
    "img_stddevs_aoi = [];\n",
    "img_max_aoi = [];\n",
    "img_min_aoi = [];\n",
    "img_range_aoi = [];\n",
    "img_q95_aoi = [];\n",
    "\n",
    "img_means_snow_aoi = [];\n",
    "img_medians_snow_aoi = [];\n",
    "img_stddevs_snow_aoi = [];\n",
    "img_max_snow_aoi = [];\n",
    "img_min_snow_aoi = [];\n",
    "img_range_snow_aoi = [];\n",
    "\n",
    "img_means_canopy_aoi = [];\n",
    "img_medians_canopy_aoi = [];\n",
    "img_stddevs_canopy_aoi = [];\n",
    "img_max_canopy_aoi = [];\n",
    "img_min_canopy_aoi = [];\n",
    "img_range_canopy_aoi = [];\n",
    "\n",
    "img_means_mixed_aoi = [];\n",
    "img_medians_mixed_aoi = [];\n",
    "img_stddevs_mixed_aoi = [];\n",
    "img_max_mixed_aoi = [];\n",
    "img_min_mixed_aoi = [];\n",
    "img_range_mixed_aoi = [];\n",
    "\n",
    "\n",
    "# I also want to keep track of the % of pixels that fall into three categories:\n",
    "fraction_snow = []; # fraction (x100 to get %) of pixels that are most likely snow\n",
    "n_snow = [];\n",
    "fraction_canopy = []; # fraction (x100 to get %)  of pixels that are most likely forest canopy\n",
    "n_canopy = [];\n",
    "fraction_mixed = []; # fraction (x100 to get %)  of pixels that are likely a mixture of snow and canopy\n",
    "n_mixed = []; # number of pixels that are likely a mixture of snow and canopy\n",
    "\n",
    "\n",
    "# make empty arrays to hold pixel sizes\n",
    "px_size = [];\n",
    "\n",
    "\n",
    "\n",
    "buffer = 0; # AOI buffer (in pixels) if we want expand/shrink AOI size\n",
    "\n",
    "# start some counters we'll need\n",
    "data_counter = 0;\n",
    "aoi_idx = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tss = 0 # known, measured snow surface temperature that we will use to correct the TIR images\n",
    "        # if Tss changes over time, this could be an array of Tss values\n",
    "    \n",
    "# set a temperature threshold to distinguish between cool (mostly snow) and warm (forest canopy) pixels:\n",
    "low_threshold = 1 * np.ones((len(aoi))) # pixels below this will be classified as snow (about 1 C)\n",
    "high_threshold = 10 * np.ones((len(aoi))) # pixels above this will be classified as canopy (about 10 C)\n",
    "\n",
    "# If we want to introduct a lag to our data correction, make sure to disable rad_correction\n",
    "#low_threshold = 1 - hist_bias\n",
    "#high_threshold = 10 - hist_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run main analysis routines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for img in images:\n",
    "\n",
    "    '''Radiometric correction using Tss:'''\n",
    "    # Perform the radiometric correction\n",
    "    if rad_correction == True:\n",
    "        if rad_correction_aoi == True:\n",
    "            # Get the original image data around the area of interest\n",
    "            # This is used to perform the radiometric correction based on the AOI portion only\n",
    "            image = data[aoi[aoi_idx][1]-buffer:aoi[aoi_idx][1]+aoi[aoi_idx][3]+buffer, \\\n",
    "                                  aoi[aoi_idx][0]-buffer:aoi[aoi_idx][0]+aoi[aoi_idx][2]+buffer, \\\n",
    "                                  img].reshape(-1);\n",
    "        else:\n",
    "            # Perform the correction based on the entire image\n",
    "            image = data[:,:,img].reshape(-1)\n",
    "        # Perform the radiometric correction of \"data[:,:,img]\",\n",
    "        # based on the histogram of \"image\", (defined above)\n",
    "        # and snow temperature reference point \"Tss\"\n",
    "        data_corrected[:,:,img], hist_offset[img-len(images)], snowpeak[img-len(images)] = radiometric_correction(data[:,:,img],image,Tss)\n",
    "    else:\n",
    "        # Or don't apply the correction:\n",
    "        data_corrected[:,:,img] = data[:,:,img]\n",
    "        hist_offset[img-len(images)] = 0\n",
    "\n",
    "    \n",
    "    '''Calculate the temperature measures we'll use later:''' \n",
    "    # make the data_aoi from data_corrected (subsetting to just our area of interest)\n",
    "    data_aoi = data_corrected[aoi[aoi_idx][1]-buffer:aoi[aoi_idx][1]+aoi[aoi_idx][3]+buffer, \\\n",
    "                          aoi[aoi_idx][0]-buffer:aoi[aoi_idx][0]+aoi[aoi_idx][2]+buffer, \\\n",
    "                          img];\n",
    "    #data_aoi = data_corrected[:,:,img]; # ignore the aoi and use the entire image\n",
    "    \n",
    "    # count the total number of pixels in this AOI:\n",
    "    n_pixels_aoi = count_px(data_aoi);\n",
    "    \n",
    "    # For the whole area of interest (AOI):\n",
    "    [i_mean, i_median, i_std, i_max, i_min, i_range] = getSummaryStats(data_aoi)\n",
    "    img_means_aoi.append(i_mean);\n",
    "    img_medians_aoi.append(i_median);\n",
    "    img_stddevs_aoi.append(i_std);\n",
    "    img_max_aoi.append(i_max);\n",
    "    img_min_aoi.append(i_min);\n",
    "    img_range_aoi.append(i_range);\n",
    "    img_q95_aoi.append(np.percentile(data_aoi,95))\n",
    "    \n",
    "    # For the snow pixels only (below low threshold):\n",
    "    idx = (data_aoi<low_threshold[img-len(images)])\n",
    "    [i_mean, i_median, i_std, i_max, i_min, i_range] = getSummaryStats(data_aoi[idx])\n",
    "    img_means_snow_aoi.append(i_mean);\n",
    "    img_medians_snow_aoi.append(i_median);\n",
    "    img_stddevs_snow_aoi.append(i_std);\n",
    "    img_max_snow_aoi.append(i_max);\n",
    "    img_min_snow_aoi.append(i_min);\n",
    "    img_range_snow_aoi.append(i_range);\n",
    "    n_pixels_snow_aoi = count_px(data_aoi[idx]); # count the number of snow pixels in this AOI:\n",
    "    n_snow.append(n_pixels_snow_aoi); # number of snow pixels\n",
    "    fraction_snow.append(n_pixels_snow_aoi/n_pixels_aoi); # % of pixels that are most likely snow\n",
    "\n",
    "    # For the canopy pixels: (two options -- include or exclude \"mixed\" pixels)\n",
    "    idx = (data_aoi>low_threshold[img-len(images)]) # (above low threshold, this includes mixed pixels)\n",
    "    #idx = (data_aoi>high_threshold[img-len(images)]) # (above high threshold, this does not include mixed pixels)\n",
    "    [i_mean, i_median, i_std, i_max, i_min, i_range] = getSummaryStats(data_aoi[idx])\n",
    "    img_means_canopy_aoi.append(i_mean);\n",
    "    img_medians_canopy_aoi.append(i_median);\n",
    "    img_stddevs_canopy_aoi.append(i_std);\n",
    "    img_max_canopy_aoi.append(i_max);\n",
    "    img_min_canopy_aoi.append(i_min);\n",
    "    img_range_canopy_aoi.append(i_range);\n",
    "    n_pixels_canopy_aoi = count_px(data_aoi[idx]); # count the number of canopy pixels in this AOI:\n",
    "    n_canopy.append(n_pixels_canopy_aoi); # number of canopy pixels\n",
    "    fraction_canopy.append(n_pixels_canopy_aoi/n_pixels_aoi); # % of pixels that are most likely snow\n",
    "    \n",
    "    # For the mixed pixels only (between thresholds):\n",
    "    idx = (data_aoi>low_threshold[img-len(images)])*(data_aoi<high_threshold[img-len(images)])\n",
    "    [i_mean, i_median, i_std, i_max, i_min, i_range] = getSummaryStats(data_aoi[idx])\n",
    "    img_means_mixed_aoi.append(i_mean);\n",
    "    img_medians_mixed_aoi.append(i_median);\n",
    "    img_stddevs_mixed_aoi.append(i_std);\n",
    "    img_max_mixed_aoi.append(i_max);\n",
    "    img_min_mixed_aoi.append(i_min);\n",
    "    img_range_mixed_aoi.append(i_range);\n",
    "    n_pixels_mixed_aoi = count_px(data_aoi[idx]); # count the number of mixed pixels in this AOI:\n",
    "    n_mixed.append(n_pixels_mixed_aoi); # number of mixed pixels\n",
    "    fraction_mixed.append(n_pixels_mixed_aoi/n_pixels_aoi); # % of pixels that are most likely snow\n",
    "\n",
    "    '''Calculate image resolution of each image:''' \n",
    "    # compute the image's approx. pixel size (GSD) (this assumes a flat surface below)\n",
    "    img_width = (img_heights[img]*np.tan(np.radians(fov_x/2)))*2 # image width (in meters)\n",
    "    img_height = (img_heights[img]*np.tan(np.radians(fov_y/2)))*2 # image height (in meters)\n",
    "    # Calculate pixel size, the average of calculated pixel width and pixel height in meters\n",
    "    px_size.append(((img_width/pix_x) + (img_height/pix_y)) / 2 );\n",
    "\n",
    "    \n",
    "    '''Plot figures:'''             \n",
    "    if plot_figures == True:\n",
    "        plt.figure(data_counter,figsize=(20,4))\n",
    "              \n",
    "        \n",
    "        ### Plot histograms\n",
    "        plt.subplot(131)\n",
    "        n_bins = 50\n",
    "        y_max = 50\n",
    "        \n",
    "        # plot full image histogram\n",
    "        value, count = create_histogram(data_corrected[:,:,img],n_bins)\n",
    "        percent_of_pixels = (count / (np.sum(count))) * 100 # calculate percentage of image from count of pixels\n",
    "        plt.plot(value,percent_of_pixels,color='tab:gray',alpha=1,linewidth=1,linestyle='--',label='Corrected Image Histogram')\n",
    "        \n",
    "        # plot aoi  histogram\n",
    "        value_aoi, count_aoi = create_histogram(data_aoi,n_bins)\n",
    "        percent_of_pixels_aoi = (count_aoi / (np.sum(count_aoi))) * 100 # calculate percentage of image from count of pixels\n",
    "        plt.plot(value_aoi,percent_of_pixels_aoi,color='tab:gray',alpha=1,linewidth=1,label='AOI Histogram')\n",
    "        \n",
    "        # plot a vertical line marking the AOI canopy mean temperature\n",
    "        #plt.plot(np.linspace(img_means_canopy_aoi[aoi_idx],img_means_canopy_aoi[aoi_idx]),np.linspace(0,2),':g',label=\"AOI Mean $T_{F}$\")\n",
    "        plt.text(img_means_canopy_aoi[aoi_idx]+.5,1,\"$\\overline{T}_{F}$: \" + str(np.round(img_means_canopy_aoi[aoi_idx],1)),color='g')\n",
    "        \n",
    "        # plot a vertical line for Tss\n",
    "        #plt.plot(np.linspace(Tss,Tss),np.linspace(0,2),':b',label=\"Snow Surface Temp.\")\n",
    "        plt.text(.5,1,\"$T_{SS}$: 0\",color='b')\n",
    "        \n",
    "        # fill in histogram with colors showing the temperature classifications\n",
    "        plt.fill_between(value_aoi, 0, percent_of_pixels_aoi, \n",
    "                         where=value_aoi<=np.ones_like(value_aoi)*1.25, \n",
    "                         facecolor='tab:blue', alpha=0.2,label='\"snow\" pixels') # < 1 C\n",
    "        plt.fill_between(value_aoi, 0, percent_of_pixels_aoi, \n",
    "                         where=((value_aoi>=np.ones_like(value_aoi)*0.75) & (value_aoi<=np.ones_like(value_aoi)*10.25)), \n",
    "                         facecolor='tab:gray', alpha=0.2,label='\"mixed\" pixels') # 1 - 10 C\n",
    "        plt.fill_between(value_aoi, 0, percent_of_pixels_aoi, \n",
    "                         where=value_aoi>=np.ones_like(value_aoi)*9.75, \n",
    "                         facecolor='tab:green', alpha=0.2,label='\"canopy\" pixels') # > 10 C\n",
    "        \n",
    "        # Plot formatting, labels\n",
    "        plt.xlim([-5,25])\n",
    "        plt.ylim([0,25])\n",
    "        title_str = \"Image: \" + str(img) + \", Alt: \" + str(img_heights[data_counter]) + \" m AGL\"\n",
    "        plt.title('Temperature Histograms\\n{}'.format(title_str))\n",
    "        plt.xlabel('Temperature (C)')\n",
    "        plt.ylabel('Percent of Image Pixels (%)')\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "\n",
    "        ### Plot the full IR image\n",
    "        ax = plt.subplot(132)\n",
    "        plt.imshow(data_corrected[:,:,img],cmap='magma',vmin=0,vmax=20)\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.ax.set_ylabel('Temperature (deg C)')\n",
    "        frame1 = plt.gca()\n",
    "        frame1.axes.get_xaxis().set_visible(False) # hide the x axis labels and tick marks\n",
    "        frame1.axes.get_yaxis().set_visible(False) # hide the y axis labels and tick marks\n",
    "        #title_str = \" mean: \" + str(np.round(np.mean(data_corrected[:,:,img]),2)) + \" min: \" + str(np.round(np.min(data_corrected[:,:,img]),2)) + \" max: \" + str(np.round(np.max(data_corrected[:,:,img]),2))\n",
    "        plt.title(title_str)\n",
    "\n",
    "        ### Plot the IR image AOI\n",
    "        ax3 = plt.subplot(133)\n",
    "        plt.imshow(data_aoi,vmin=0,vmax=20,cmap='magma')\n",
    "        plt.title(str(aoi[aoi_idx][2]) +\" x \"+ str(aoi[aoi_idx][3]) + \"px\")\n",
    "        frame2 = plt.gca()\n",
    "        frame2.axes.get_xaxis().set_visible(False) # hide the x axis labels and tick marks\n",
    "        frame2.axes.get_yaxis().set_visible(False) # hide the y axis labels and tick marks\n",
    "        title_str = \"AOI: Image Resolution: \" + str(np.round(px_size[aoi_idx],2)) + \" m \\n\"\\\n",
    "                    \"mean: \" + str(np.round(img_means_aoi[aoi_idx],2)) + \\\n",
    "                    \", min: \" + str(np.round(img_min_aoi[aoi_idx],2)) + \\\n",
    "                    \", max: \" + str(np.round(img_max_aoi[aoi_idx],2))\n",
    "        plt.title(title_str)\n",
    "        rect = patches.Rectangle((aoi[aoi_idx][0],aoi[aoi_idx][1]),aoi[aoi_idx][2],aoi[aoi_idx][3],\\\n",
    "                                 linewidth=1,edgecolor='w',facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    \n",
    "    aoi_idx = aoi_idx + 1;\n",
    "\n",
    "    if save_figures == True:\n",
    "        plt.savefig(r'plots\\output\\img_' + str(img) + '.png')\n",
    "    data_counter = data_counter + 1;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a histogram of how far the measured snow temperature was from the expected Tss=0\n",
    "hist_bias  = hist_offset[~np.isnan(hist_offset)]\n",
    "rmse_tss, mean_bias_tss = rmse(Tss,hist_bias)\n",
    "count, value = np.histogram(hist_bias, bins=20); \n",
    "new_value = [(i+value[idx+1])/2 for idx, i in enumerate(value) if idx < len(value)-1]\n",
    "value = new_value\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(10,3))\n",
    "ax1.bar(value, count, width=0.2, color='k', edgecolor='k')\n",
    "ax1.set_title('Temperature Biases Histogram')\n",
    "ax1.set_ylabel('Number of Images');\n",
    "ax1.set_xlabel('Temperature Bias (C)');\n",
    "\n",
    "# Make a time series plot of the biases\n",
    "ax2.plot(hist_bias,'.-k',markersize=10)\n",
    "ax2.set_title('Temperature Biases over Time')\n",
    "ax2.set_xlabel('Image Sequence Number');\n",
    "ax2.set_ylabel('Temperature Bias (C)');\n",
    "\n",
    "#plt.savefig('sagehen_biases.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### Look at the change in mixed pixel fraction vs image resolution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot change in fraction snow, canopy, mixed pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0,figsize=(7,4))\n",
    "plt.plot(px_size,fraction_snow,'b.',marker='$*$',label=\"snow pixel fraction\")\n",
    "plt.plot(px_size,fraction_canopy,'g^',label=\"canopy pixel fraction\")\n",
    "plt.plot(px_size,fraction_mixed,'k.',markersize=10,label=\"mixed pixel fraction\")\n",
    "plt.ylim((0,1))\n",
    "plt.xlabel('Image Resolution (pixel size, m)')\n",
    "plt.ylabel('Fraction of Image')\n",
    "plt.grid(b=True, which='major', color='grey', linestyle=':')\n",
    "plt.title('Snow, Canopy, and Mixed Pixel Fractions')\n",
    "plt.title('Fraction of image in each class')\n",
    "plt.legend()\n",
    "#plt.savefig(r'plots\\PaperFigures\\sagehen_allPixelFraction.png',\n",
    "#            transparent=False, dpi=300)\n",
    "\n",
    "\n",
    "print(\"Fraction Canopy Linear Regression\")\n",
    "quick_lin(px_size,fraction_canopy,'g')\n",
    "print(\"Fraction Mixed Linear Regression\")\n",
    "quick_lin(px_size,fraction_mixed,'k')\n",
    "print(\"Fraction snow Linear Regression\")\n",
    "quick_lin(px_size,fraction_snow,'b')\n",
    "print(\"\\nPixel sizes: \",px_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mixed Pixel Fraction vs Image Resolution\n",
    "(Figure S4a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0,figsize=(6,6))\n",
    "\n",
    "mixed_fraction = np.array(fraction_mixed)/np.array(fraction_canopy)\n",
    "\n",
    "plt.plot(px_size,mixed_fraction,'.',markersize=10,label=\"mixed pixel fraction\",c='tab:blue')\n",
    "\n",
    "# Linear regression:\n",
    "[slope, intercept, r, p, SE] = stats.linregress(px_size,mixed_fraction)\n",
    "print([slope, intercept, r, p, SE])\n",
    "x = np.linspace(np.min(px_size),np.max(px_size),2)\n",
    "plt.plot(x,x*slope + intercept,':',c='tab:blue')\n",
    "\n",
    "\n",
    "\n",
    "#plt.ylim((0,1))\n",
    "plt.xlabel('Image Resolution (pixel size, m)')\n",
    "plt.ylabel('Fraction of Canopy Pixels')\n",
    "#plt.grid(b=True, which='major', color='grey', linestyle=':')\n",
    "#plt.title('Snow, Canopy, and Mixed Pixel Fractions')\n",
    "plt.title('Mixed Pixel Fraction of Canopy')\n",
    "#plt.legend()\n",
    "###\n",
    "plt.xlim((0,0.))\n",
    "plt.xticks(np.arange(0, 0.21, 0.05))\n",
    "plt.ylim((0,.4))\n",
    "###\n",
    "#plt.savefig(r'plots\\davos2_mixedPixelFraction_.png',\n",
    "#            transparent=True, dpi=300)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### Test for significant change in measured temperatures with image resolution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select image set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select image set to use, assign mean, range, stddev values etc.\n",
    "\n",
    "if sagehen == True:\n",
    "    # Sagehen:\n",
    "    idx = list(range(0,42))\n",
    "\n",
    "if sagehen == False:\n",
    "    #  Davos:\n",
    "    idx = list(range(0,70)) # Davos 1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select temperature class to test:\n",
    "(Figure S4b) (options: all, canopy, snow, mixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Tcanopy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Assign Tcanopy values:\n",
    "means = np.array(img_means_canopy_aoi[idx[0]:idx[-1]]);\n",
    "mins = np.array(img_min_canopy_aoi[idx[0]:idx[-1]]);\n",
    "maxs = np.array(img_max_canopy_aoi[idx[0]:idx[-1]]);\n",
    "stddevs = np.array(img_stddevs_canopy_aoi[idx[0]:idx[-1]]);\n",
    "px_sizes = px_size[idx[0]:idx[-1]]\n",
    "\n",
    "n = len(means) # number of image samples we have\n",
    "std_error = 0 # relative error between pixels w/in a single image\n",
    "std_error_mean = 1 # absolute error between individual images of the same scene\n",
    "\n",
    "test_for_T_change(px_sizes,means,stddevs,mins,maxs,std_error_mean,std_error,'tab:blue',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Monte Carlo tests on the significance of measurement error to linear regression results:\n",
    "s = 1 # introduce a measurement error normally distributed within +/1 deg C\n",
    "m = 100 # number of test runs to perform\n",
    "\n",
    "# If i'm doing a 2-sided test with alpha=5%, then reject the null (no slope) if p <= alpha/2\n",
    "msmtErrorTest_plots(px_sizes,means,s,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mean vs mixed canopy fraction\n",
    "plt.plot(fraction_mixed[0:],means,'.k')\n",
    "plt.ylabel('mean')\n",
    "plt.xlabel('fmixed')\n",
    "quick_lin(fraction_mixed[0:],means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Tsnow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Assign Tsnow values:\n",
    "means = np.array(img_means_snow_aoi[idx[0]:idx[-1]]);\n",
    "mins= np.array(img_min_snow_aoi[idx[0]:idx[-1]]);\n",
    "maxs= np.array(img_max_snow_aoi[idx[0]:idx[-1]]);\n",
    "stddevs = np.array(img_stddevs_snow_aoi[idx[0]:idx[-1]]);\n",
    "px_sizes = px_size[idx[0]:idx[-1]]\n",
    "\n",
    "n = len(means) # number of image samples we have\n",
    "std_error = 0 # relative error between pixels w/in a single image\n",
    "std_error_mean = 1 # absolute error between individual images of the same scene\n",
    "\n",
    "test_for_T_change(px_sizes,means,stddevs,mins,maxs,std_error_mean,std_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Monte Carlo tests on the significance of measurement error to linear regression results:\n",
    "s = 1 # measurement error in +/1 deg C\n",
    "m = 100 # number of test runs with normally distributed random error to perform\n",
    "\n",
    "# If i'm doing a 2-sided test with alpha=5%, then reject the null (no slope) if p <= alpha/2\n",
    "msmtErrorTest_plots(px_sizes,means,s,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Tmixed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Assign Tmixed values:\n",
    "means = np.array(img_means_mixed_aoi[idx[0]:idx[-1]]);\n",
    "mins = np.array(img_min_mixed_aoi[idx[0]:idx[-1]]);\n",
    "maxs = np.array(img_max_mixed_aoi[idx[0]:idx[-1]]);\n",
    "stddevs = np.array(img_stddevs_mixed_aoi[idx[0]:idx[-1]]);\n",
    "px_sizes = px_size[idx[0]:idx[-1]]\n",
    "\n",
    "n = len(means) # number of image samples we have\n",
    "std_error = 0 # relative error between pixels w/in a single image\n",
    "std_error_mean = 1 # absolute error between individual images of the same scene\n",
    "\n",
    "test_for_T_change(px_sizes,means,stddevs,mins,maxs,std_error_mean,std_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Monte Carlo tests on the significance of measurement error to linear regression results:\n",
    "s = 1 # measurement error in +/1 deg C\n",
    "m = 100 # number of test runs with normally distributed random error to perform\n",
    "\n",
    "# If i'm doing a 2-sided test with alpha=5%, then reject the null (no slope) if p <= alpha/2\n",
    "msmtErrorTest_plots(px_sizes,means,s,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test all temperatures within the area of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Assign all AOI T values:\n",
    "means = np.array(img_means_aoi[idx[0]:idx[-1]]);\n",
    "mins = np.array(img_min_aoi[idx[0]:idx[-1]]);\n",
    "maxs = np.array(img_max_aoi[idx[0]:idx[-1]]);\n",
    "stddevs = np.array(img_stddevs_aoi[idx[0]:idx[-1]]);\n",
    "px_sizes = px_size[idx[0]:idx[-1]]\n",
    "\n",
    "n = len(means) # number of image samples we have\n",
    "std_error = 0 # relative error between pixels w/in a single image\n",
    "std_error_mean = 1 # absolute error between individual images of the same scene\n",
    "\n",
    "test_for_T_change(px_sizes,means,stddevs,mins,maxs,std_error_mean,std_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Monte Carlo tests on the significance of measurement error to linear regression results:\n",
    "s = 1 # measurement error in +/1 deg C\n",
    "m = 100 # number of test runs with normally distributed random error to perform\n",
    "\n",
    "# If i'm doing a 2-sided test with alpha=5%, then reject the null (no slope) if p <= alpha/2\n",
    "msmtErrorTest_plots(px_sizes,maxs,s,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "#### Retrieving surface temperatures from selected images at Sagehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a horizontal cross section across a couple images to compre before and after data correction\n",
    "\n",
    "n = 363; # the row we want to draw a transect line across\n",
    "\n",
    "for i in [5,42]: # look at a couple images (image #5, and #42)\n",
    "    fig, (ax_img, ax_temp) = plt.subplots(1,2,figsize=(14,5))\n",
    "    \n",
    "    # show the image itself\n",
    "    ax_img.imshow(data_corrected[:,:,i],cmap='magma',vmin=-2,vmax=20,origin='upper');\n",
    "    ax_img.plot(list(range(0,640)),np.ones(len(list(range(0,640))))*n,':w',label='Transect Line')\n",
    "    ax_img.legend(loc='upper left')\n",
    "    ax_img.set_xlabel('Image Pixel Column')\n",
    "    ax_img.set_ylabel('Image Pixel Row')\n",
    "    ax_img.set_title('TIR Image #{}'.format(i))\n",
    "    \n",
    "    # show a temperature transect across the image\n",
    "    ax_temp.plot(data[n,:,i],'--',c='tab:gray',label='Original Surface Temp.');\n",
    "    ax_temp.plot(data_corrected[n,:,i],'-k',label='Corrected Surface Temp.');\n",
    "    ax_temp.plot(list(range(0,640)),np.zeros(len(list(range(0,640)))),':k',label='$T_{ss}$ = 0 $\\degree$C')\n",
    "    ax_temp.set_ylabel('Surface Temperature ($\\degree$C)')\n",
    "    ax_temp.set_xlabel('Image Pixel Column')\n",
    "    ax_temp.set_xlim((0,640))\n",
    "    ax_temp.set_title('Temperature Along a Transect Line')\n",
    "    ax_temp.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Sagehen: What is the water surface temperature of Sagehen Creek?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_images = False\n",
    "\n",
    "# image pixel coordinates of a visible portion of Sagehen Creek\n",
    "x = [150,150,160,160,175,190,180,180,190,190,200,210,210,210,210,220,220,210,210,210,220,220,210]\n",
    "y = [410,420,450,450,450,420,430,430,420,430,410,420,420,420,420,400,400,400,400,400,390,400,390]\n",
    "\n",
    "w = 15; # window size in px to grab water temperature from within\n",
    "water_temps = [];\n",
    "water_temps_nc = [];\n",
    "\n",
    "for i in range(22,45):\n",
    "    idx = i-22;\n",
    "    if plot_images == True:\n",
    "        plt.figure(idx,figsize=(12,8))\n",
    "        plt.imshow(data_corrected[:,:,i],cmap='magma',vmin=-2,vmax=20);\n",
    "        plt.colorbar()\n",
    "        plt.tick_params(axis='both', left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "        plt.plot(x[idx],y[idx],'.w',label='Sagehen Creek AOI')\n",
    "        plt.plot(x[idx]+w,y[idx],'|w')\n",
    "        plt.plot(x[idx],y[idx]+w,'_w')\n",
    "        plt.plot(x[idx],y[idx]-w,'_w')\n",
    "        plt.plot(x[idx]-w,y[idx],'|w')\n",
    "        plt.legend(loc='lower right')\n",
    "\n",
    "    water_temps.append(data_corrected[y[idx]-w:y[idx]+w,x[idx]-w:x[idx]+w,i])\n",
    "    water_temps_nc.append(data[y[idx]-w:y[idx]+w,x[idx]-w:x[idx]+w,i])\n",
    "    \n",
    "    \n",
    "cleanhist(water_temps,n_bins=500)\n",
    "print(\"Standard Deviation: \" + str(np.round(np.std(water_temps),2)))\n",
    "plt.xlim((3,7))\n",
    "#plt.savefig(r'\\plots\\\\'+str(i)+'_hist.png',\n",
    "#            transparent=False, dpi=300)\n",
    "\n",
    "print('points',len(x))\n",
    "# Calculate RMSE against USGS gage water temperature:\n",
    "tw = 4.6 # mean USGS gage water temperature @ 15:00 - 16:00\n",
    "tw_rmse, mean_bias = rmse(tw, np.array(water_temps))\n",
    "print('RMSE:',tw_rmse)\n",
    "print('Mean Bias:',mean_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Sagehen: What is the snow surface temperature within this meadow?\n",
    "We can pick out a small area from each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_images = False\n",
    "\n",
    "# image pixel coordinates of a poriton of snow-covered meadow\n",
    "x = [150, 150, 150, 150, 150, 200, 200, 200, 180, 190, 200, 200, 200, 200, 190, 210, 190,\n",
    "     550, 550, 550, 550, 550, 550, 550, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
    "     500, 500, 500, 500, 500, 500]\n",
    "y = [480, 480, 460, 460, 470, 450, 440, 420, 320, 310, 310, 300, 290, 290, 280, 300, 230, \n",
    "     300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300,\n",
    "     300, 300, 250, 250, 250, 250]\n",
    "\n",
    "w = 30; # window size in px to grab meadow snow temperature from within\n",
    "meadow_snow_temps = []; # to hold the corrected data\n",
    "meadow_snow_temps_nc = []; # to hold data that is \"not corrected\"\n",
    "\n",
    "for i in range(5,45):\n",
    "    idx = i-5;\n",
    "    if plot_images == True:\n",
    "        plt.figure(idx,figsize=(8,5))\n",
    "        plt.imshow(data_corrected[:,:,i],cmap='magma',vmin=-2,vmax=20);\n",
    "        plt.colorbar()\n",
    "        #plt.tick_params(axis='both', left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "        plt.plot(x[idx],y[idx],'.w',label='Sagehen Creek AOI')\n",
    "        plt.plot(x[idx]+w,y[idx],'|w')\n",
    "        plt.plot(x[idx],y[idx]+w,'_w')\n",
    "        plt.plot(x[idx],y[idx]-w,'_w')\n",
    "        plt.plot(x[idx]-w,y[idx],'|w')\n",
    "        plt.legend(loc='lower right')\n",
    "\n",
    "    meadow_snow_temps.append(data_corrected[y[idx]-w:y[idx]+w,x[idx]-w:x[idx]+w,i])\n",
    "    meadow_snow_temps_nc.append(data[y[idx]-w:y[idx]+w,x[idx]-w:x[idx]+w,i])\n",
    "    \n",
    "    \n",
    "cleanhist(meadow_snow_temps,n_bins=200)\n",
    "print(\"Standard Deviation: \" + str(np.round(np.std(meadow_snow_temps),2)))\n",
    "plt.xlim((-2,2))\n",
    "#plt.savefig(r'plots\\\\'+str(i)+'_hist.png',\n",
    "#            transparent=False, dpi=300)\n",
    "\n",
    "print('points',len(x))\n",
    "# Calculate RMSE against our melting Tss=0\n",
    "tss = 0\n",
    "tss_rmse, mean_bias = rmse(tss, np.array(meadow_snow_temps))\n",
    "print('RMSE:',tss_rmse)\n",
    "print('Mean Bias:',mean_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or sample along a transect in a single image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a horizontal cross section across a couple images to compre before and after data correction\n",
    "\n",
    "x = np.linspace(240,520,50);\n",
    "y = np.linspace(10,480,50);\n",
    "w = 10\n",
    "\n",
    "for i in [42]: # look at a couple images (image #5, and #42)\n",
    "    fig, (ax_img, ax_temp) = plt.subplots(1,2,figsize=(14,5))\n",
    "    \n",
    "    # show the image itself\n",
    "    ax_img.imshow(data_corrected[:,:,i],cmap='magma',vmin=-2,vmax=20,origin='upper');\n",
    "    ax_img.plot(x,y,':w',label='Transect Line')\n",
    "    ax_img.legend(loc='upper left')\n",
    "    ax_img.set_xlabel('Image Pixel Column')\n",
    "    ax_img.set_ylabel('Image Pixel Row')\n",
    "    ax_img.set_title('TIR Image #{}'.format(i))\n",
    "    \n",
    "    # get the temperature across the transect line\n",
    "    snow_temps_nc = [];\n",
    "    snow_temps = [];\n",
    "    for ii in range(0,x.shape[0]):\n",
    "        snow_temps_nc.append(data[int(y[ii])-w:int(y[ii])+w,int(x[ii])-w:int(x[ii])+w,i])\n",
    "        snow_temps.append(data_corrected[int(y[ii])-w:int(y[ii])+w,int(x[ii])-w:int(x[ii])+w,i])\n",
    "    snow_temps_nc = np.array(snow_temps_nc).ravel()\n",
    "    snow_temps = np.array(snow_temps).ravel()\n",
    "    \n",
    "    # show a temperature transect across the image\n",
    "    ax_temp.plot(snow_temps_nc,'--',c='tab:gray',label='Original Surface Temp.');\n",
    "    ax_temp.plot(snow_temps,'-k',alpha=1,label='Corrected Surface Temp.');\n",
    "    ax_temp.plot([0,snow_temps.shape[0]],[0,0],':k',label='$T_{ss}$ = 0 $\\degree$C')\n",
    "    ax_temp.set_ylabel('Surface Temperature ($\\degree$C)')\n",
    "    ax_temp.set_xlabel('Sample Count')\n",
    "    ax_temp.set_ylim((-5,25))\n",
    "    ax_temp.set_title('Temperature Along a Transect Line')\n",
    "    ax_temp.legend()\n",
    "    \n",
    "# Plot a histogram\n",
    "cleanhist(snow_temps,n_bins=50)\n",
    "plt.title('Transect Line Temperature Histogram')\n",
    "plt.xlim((-3,3))\n",
    "\n",
    "#plt.savefig(r'plots\\\\'+str(i)+'_hist.png',\n",
    "#            transparent=False, dpi=300)\n",
    "\n",
    "\n",
    "print(\"Standard Deviation: \" + str(np.round(np.std(snow_temps),2)))\n",
    "# Calculate RMSE against mean fixed radiometer snow surface temperature:\n",
    "tss = 0\n",
    "tss_rmse, mean_bias = rmse(tss, np.array(snow_temps))\n",
    "print('RMSE:',tss_rmse)\n",
    "print('Mean Bias:',mean_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### What are some other forest canopy temperatures around here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a horizontal cross section across a couple images to compre before and after data correction\n",
    "\n",
    "x = np.linspace(240,150,50);\n",
    "y = np.linspace(170,340,50);\n",
    "w = 1\n",
    "\n",
    "for i in [42]: # look at a couple images (image #5, and #42)\n",
    "    fig, (ax_img, ax_temp) = plt.subplots(1,2,figsize=(14,5))\n",
    "    \n",
    "    # show the image itself\n",
    "    ax_img.imshow(data_corrected[:,:,i],cmap='magma',vmin=-2,vmax=20,origin='upper');\n",
    "    ax_img.plot(x,y,':w',label='Transect Line')\n",
    "    ax_img.legend(loc='upper left')\n",
    "    ax_img.set_xlabel('Image Pixel Column')\n",
    "    ax_img.set_ylabel('Image Pixel Row')\n",
    "    ax_img.set_title('TIR Image #{}'.format(i))\n",
    "    \n",
    "    # get the temperature across the transect line\n",
    "    snow_temps_nc = [];\n",
    "    snow_temps = [];\n",
    "    for ii in range(0,x.shape[0]):\n",
    "        snow_temps_nc.append(data[int(y[ii])-w:int(y[ii])+w,int(x[ii])-w:int(x[ii])+w,i])\n",
    "        snow_temps.append(data_corrected[int(y[ii])-w:int(y[ii])+w,int(x[ii])-w:int(x[ii])+w,i])\n",
    "    snow_temps_nc = np.array(snow_temps_nc).ravel()\n",
    "    snow_temps = np.array(snow_temps).ravel()\n",
    "    \n",
    "    # show a temperature transect across the image\n",
    "    ax_temp.plot(snow_temps_nc,'--',c='tab:gray',label='Original Surface Temp.');\n",
    "    ax_temp.plot(snow_temps,'-k',alpha=1,label='Corrected Surface Temp.');\n",
    "    ax_temp.plot([0,snow_temps.shape[0]],[0,0],':k',label='$T_{ss}$ = 0 $\\degree$C')\n",
    "    ax_temp.set_ylabel('Surface Temperature ($\\degree$C)')\n",
    "    ax_temp.set_xlabel('Sample Count')\n",
    "    ax_temp.set_ylim((-5,25))\n",
    "    ax_temp.set_title('Temperature Along a Transect Line')\n",
    "    ax_temp.legend()\n",
    "    \n",
    "# Plot a histogram\n",
    "cleanhist(snow_temps,n_bins=100)\n",
    "plt.title('Transect Line Temperature Histogram')\n",
    "#plt.xlim((-3,3))\n",
    "\n",
    "#plt.savefig(r'plots\\\\'+str(i)+'_hist.png',\n",
    "#            transparent=False, dpi=300)\n",
    "\n",
    "\n",
    "print(\"Standard Deviation: \" + str(np.round(np.std(snow_temps),2)))\n",
    "# Calculate RMSE against mean fixed radiometer Tf:\n",
    "tss = 16.9\n",
    "tss_rmse, mean_bias = rmse(tss, np.array(snow_temps))\n",
    "print('RMSE:',tss_rmse)\n",
    "print('Mean Bias:',mean_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the canopy temperature of the instrumented tree?\n",
    "... at the highest resolution available from the drone imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a horizontal cross section across a couple images to compre before and after data correction\n",
    "\n",
    "x = np.linspace(150,100,50);\n",
    "y = np.linspace(300,350,50);\n",
    "w = 1\n",
    "\n",
    "for i in [5]: # look at a couple images (image #5, and #42)\n",
    "    fig, (ax_img, ax_temp) = plt.subplots(1,2,figsize=(14,5))\n",
    "    \n",
    "    # show the image itself\n",
    "    ax_img.imshow(data_corrected[:,:,i],cmap='magma',vmin=-2,vmax=20,origin='upper');\n",
    "    ax_img.plot(x,y,':w',label='Transect Line')\n",
    "    ax_img.legend(loc='upper left')\n",
    "    ax_img.set_xlabel('Image Pixel Column')\n",
    "    ax_img.set_ylabel('Image Pixel Row')\n",
    "    ax_img.set_title('TIR Image #{}'.format(i))\n",
    "    \n",
    "    # get the temperature across the transect line\n",
    "    snow_temps_nc = [];\n",
    "    snow_temps = [];\n",
    "    for ii in range(0,x.shape[0]):\n",
    "        snow_temps_nc.append(data[int(y[ii])-w:int(y[ii])+w,int(x[ii])-w:int(x[ii])+w,i])\n",
    "        snow_temps.append(data_corrected[int(y[ii])-w:int(y[ii])+w,int(x[ii])-w:int(x[ii])+w,i])\n",
    "    snow_temps_nc = np.array(snow_temps_nc).ravel()\n",
    "    snow_temps = np.array(snow_temps).ravel()\n",
    "    \n",
    "    # show a temperature transect across the image\n",
    "    ax_temp.plot(snow_temps_nc,'--',c='tab:gray',label='Original Surface Temp.');\n",
    "    ax_temp.plot(snow_temps,'-k',alpha=1,label='Corrected Surface Temp.');\n",
    "    ax_temp.plot([0,snow_temps.shape[0]],[0,0],':k',label='$T_{ss}$ = 0 $\\degree$C')\n",
    "    ax_temp.set_ylabel('Surface Temperature ($\\degree$C)')\n",
    "    ax_temp.set_xlabel('Sample Count')\n",
    "    ax_temp.set_ylim((-2,30))\n",
    "    ax_temp.set_title('Temperature Along a Transect Line')\n",
    "    ax_temp.legend()\n",
    "    \n",
    "# Plot a histogram\n",
    "cleanhist(snow_temps,n_bins=50)\n",
    "plt.title('Transect Line Temperature Histogram')\n",
    "#plt.xlim((-3,3))\n",
    "\n",
    "#plt.savefig(r'plots\\\\'+str(i)+'_hist.png',\n",
    "#            transparent=False, dpi=300)\n",
    "\n",
    "\n",
    "print(\"Standard Deviation: \" + str(np.round(np.std(snow_temps),2)))\n",
    "# Calculate RMSE against mean fixed radiometer Tf:\n",
    "tss = 16.9\n",
    "tss_rmse, mean_bias = rmse(tss, np.array(snow_temps))\n",
    "print('RMSE:',tss_rmse)\n",
    "print('Mean Bias:',mean_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a lower resolution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a horizontal cross section across a couple images to compre before and after data correction\n",
    "\n",
    "x = np.linspace(470,490,50);\n",
    "y = np.linspace(200,180,50);\n",
    "w = 1\n",
    "\n",
    "for i in [42]: # look at a couple images (image #5, and #42)\n",
    "    fig, (ax_img, ax_temp) = plt.subplots(1,2,figsize=(14,5))\n",
    "    \n",
    "    # show the image itself\n",
    "    ax_img.imshow(data_corrected[:,:,i],cmap='magma',vmin=-2,vmax=20,origin='upper');\n",
    "    ax_img.plot(x,y,':w',label='Transect Line')\n",
    "    ax_img.legend(loc='upper left')\n",
    "    ax_img.set_xlabel('Image Pixel Column')\n",
    "    ax_img.set_ylabel('Image Pixel Row')\n",
    "    ax_img.set_title('TIR Image #{}'.format(i))\n",
    "    \n",
    "    # get the temperature across the transect line\n",
    "    snow_temps_nc = [];\n",
    "    snow_temps = [];\n",
    "    for ii in range(0,x.shape[0]):\n",
    "        snow_temps_nc.append(data[int(y[ii])-w:int(y[ii])+w,int(x[ii])-w:int(x[ii])+w,i])\n",
    "        snow_temps.append(data_corrected[int(y[ii])-w:int(y[ii])+w,int(x[ii])-w:int(x[ii])+w,i])\n",
    "    snow_temps_nc = np.array(snow_temps_nc).ravel()\n",
    "    snow_temps = np.array(snow_temps).ravel()\n",
    "    \n",
    "    # show a temperature transect across the image\n",
    "    ax_temp.plot(snow_temps_nc,'--',c='tab:gray',label='Original Surface Temp.');\n",
    "    ax_temp.plot(snow_temps,'-k',alpha=1,label='Corrected Surface Temp.');\n",
    "    ax_temp.plot([0,snow_temps.shape[0]],[0,0],':k',label='$T_{ss}$ = 0 $\\degree$C')\n",
    "    ax_temp.set_ylabel('Surface Temperature ($\\degree$C)')\n",
    "    ax_temp.set_xlabel('Sample Count')\n",
    "    ax_temp.set_ylim((-5,25))\n",
    "    ax_temp.set_title('Temperature Along a Transect Line')\n",
    "    ax_temp.legend()\n",
    "    \n",
    "# Plot a histogram\n",
    "cleanhist(snow_temps,n_bins=20)\n",
    "plt.title('Transect Line Temperature Histogram')\n",
    "#plt.xlim((-3,3))\n",
    "\n",
    "#plt.savefig(r'plots\\\\'+str(i)+'_hist.png',\n",
    "#            transparent=False, dpi=300)\n",
    "\n",
    "\n",
    "print(\"Standard Deviation: \" + str(np.round(np.std(snow_temps),2)))\n",
    "# Calculate RMSE against mean fixed radiometer Tf:\n",
    "tss = 16.9\n",
    "tss_rmse, mean_bias = rmse(tss, np.array(snow_temps))\n",
    "print('RMSE:',tss_rmse)\n",
    "print('Mean Bias:',mean_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### Image Upscaling (Aggregation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Resample all images to a target resolution (matching that of the aircraft)\n",
    "### Use these to compare mean and standard deviation against the aircraft images\n",
    "### mean and standard deviations (for full image, or AOI only)\n",
    "\n",
    "target_res = 1.0 # target resolution in meters, approximate\n",
    "buffer=0\n",
    "plot_images = False;\n",
    "save_images = False;\n",
    "\n",
    "original_means = [];\n",
    "original_stddevs = [];\n",
    "downscaled_means = [];\n",
    "downscaled_stddevs = [];\n",
    "\n",
    "for im_index in range(0,len(images)):\n",
    "    \n",
    "    im_num = images[im_index]\n",
    "    \n",
    "    # Compute these based on entire image:\n",
    "    #data_aoi = data_corrected[:,:,im_num];\n",
    "    \n",
    "    # Option to compute these based on the AOI only:\n",
    "    buffer = 10\n",
    "    data_aoi = data_corrected[aoi[im_index][1]-buffer:aoi[im_index][1]+aoi[im_index][3]+buffer, \\\n",
    "                              aoi[im_index][0]-buffer:aoi[im_index][0]+aoi[im_index][2]+buffer, \\\n",
    "                              im_num]\n",
    "    \n",
    "    \n",
    "    original_means.append(np.mean(data_aoi));\n",
    "    original_stddevs.append(np.std(data_aoi));\n",
    "    \n",
    "    \n",
    "    if plot_images == True:\n",
    "        plt.figure(im_index+10,figsize=(18,5))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(data_aoi,cmap='magma',vmin=0,vmax=20)\n",
    "        plt.title('Original Image ' + str(np.round(px_size[im_index],2)) + 'm/px')\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(lower_res,cmap='magma',vmin=0,vmax=20)\n",
    "        plt.title('Gaussian Upscaled to ' + str(np.round(new_res,2)) + 'm/px')\n",
    "        plt.colorbar()\n",
    "        if save_images == True:\n",
    "            plt.savefig(r'plots\\resampling\\images' + str(im_index) + \".png\")\n",
    "    \n",
    "    ##### use OpenCV for gaussian pyramid:\n",
    "    steps = int(np.round(np.log(target_res/px_size[im_index])/(np.log(2))))\n",
    "    new_res = px_size[im_index] * 2**steps\n",
    "    lower_res = data_aoi\n",
    "    for i in range(0,steps):\n",
    "        lower_res = cv2.pyrDown(lower_res)\n",
    "\n",
    "    downscaled_means.append(np.mean(lower_res));\n",
    "    downscaled_stddevs.append(np.std(lower_res));\n",
    "    \n",
    "\n",
    "# plot a sample image pair\n",
    "plt.figure(5,figsize=(10,4))\n",
    "plt.subplot(121)\n",
    "plt.imshow(data_aoi,cmap='magma',vmin=0, vmax=20)\n",
    "plt.title(\"Original Image\")\n",
    "plt.colorbar()\n",
    "plt.subplot(122)\n",
    "plt.imshow(lower_res,cmap='magma',vmin=0, vmax=20)\n",
    "plt.title(\"Upscaled Image\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "    \n",
    "# compare histograms of image means before and after downscaling\n",
    "plt.figure(4,figsize=(7,7))\n",
    "plt.subplot(221)\n",
    "count, value = np.histogram(original_means); \n",
    "# take the bin edges (value) and change them to bin centers\n",
    "# so that number of \"count\" = number of \"value\"\n",
    "new_value = [(i+value[idx+1])/2 for idx, i in enumerate(value) if idx < len(value)-1]\n",
    "value = new_value\n",
    "plt.plot(value,count,'-k', label='original means')\n",
    "count, value = np.histogram(downscaled_means); \n",
    "# take the bin edges (value) and change them to bin centers\n",
    "# so that number of \"count\" = number of \"value\"\n",
    "new_value = [(i+value[idx+1])/2 for idx, i in enumerate(value) if idx < len(value)-1]\n",
    "value = new_value\n",
    "plt.plot(value,count,'--k',label='upscaled means')\n",
    "plt.legend()\n",
    "\n",
    "# compare histograms of image standard deviations before and after downscaling\n",
    "plt.subplot(222)\n",
    "count, value = np.histogram(original_stddevs); \n",
    "# take the bin edges (value) and change them to bin centers\n",
    "# so that number of \"count\" = number of \"value\"\n",
    "new_value = [(i+value[idx+1])/2 for idx, i in enumerate(value) if idx < len(value)-1]\n",
    "value = new_value\n",
    "plt.plot(value,count,'-k', label='original stddevs')\n",
    "count, value = np.histogram(downscaled_stddevs); \n",
    "# take the bin edges (value) and change them to bin centers\n",
    "# so that number of \"count\" = number of \"value\"\n",
    "new_value = [(i+value[idx+1])/2 for idx, i in enumerate(value) if idx < len(value)-1]\n",
    "value = new_value\n",
    "plt.plot(value,count,'--k',label='upscaled stddevs')\n",
    "plt.legend()\n",
    "    \n",
    "# look at how the original mean compares with downscaled image mean\n",
    "plt.subplot(223)\n",
    "plt.scatter(original_means,downscaled_means,s=2,c='k')\n",
    "plt.plot(np.linspace(np.min(original_means)-1,np.max(original_means)+1,10),np.linspace(np.min(original_means)-1,np.max(original_means)+1,10),':b')\n",
    "plt.xlabel('Original Mean (C)')\n",
    "plt.ylabel('Gaussian Upscaled Mean (C)')\n",
    "plt.title('Image Means')\n",
    "\n",
    "# look at how the original stddev compares with downscaled image stddev\n",
    "plt.subplot(224)\n",
    "plt.scatter(original_stddevs,downscaled_stddevs,s=2,c='k')\n",
    "plt.plot(np.linspace(np.min(original_stddevs)-1,np.max(original_stddevs)+1,10),np.linspace(np.min(original_stddevs)-1,np.max(original_stddevs)+1,10),':b')\n",
    "plt.xlabel('Original Standard Deviation (C)')\n",
    "plt.ylabel('Gaussian Upscaled Standard Deviation (C)')\n",
    "plt.title('Image Standard Deviations')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
